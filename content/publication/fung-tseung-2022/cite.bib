@article{fung_tseung_2022,
 abstract = {Multilevel data are prevalent in many real-world applications. However, it remains an open research problem to identify and justify a class of models that flexibly capture a wide range of multilevel data. Motivated by the versatility of the mixture of experts (MoE) models in fitting regression data, in this article we extend upon the MoE and study a class of mixed MoE (MMoE) models for multilevel data. Under some regularity conditions, we prove that the MMoE is dense in the space of any continuous mixed effects models in the sense of weak convergence. As a result, the MMoE has a potential to accurately resemble almost all characteristics inherited in multilevel data, including the marginal distributions, dependence structures, regression links, random intercepts and random slopes. In a particular case where the multilevel data is hierarchical, we further show that a nested version of the MMoE universally approximates a broad range of dependence structures of the random effects among different factor levels.},
 author = {Fung, Tsz Chai and Tseung, Spark C.},
 copyright = {arXiv.org perpetual, non-exclusive license},
 doi = {10.48550/ARXIV.2209.15207},
 keywords = {Statistics Theory (math.ST), Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Methodology (stat.ME), FOS: Mathematics, FOS: Mathematics, FOS: Computer and information sciences, FOS: Computer and information sciences},
 publisher = {arXiv},
 title = {Mixture of experts models for multilevel data: modelling framework and approximation theory},
 url = {https://arxiv.org/abs/2209.15207},
 year = {2022}
}

