---
# Documentation: https://wowchemy.com/docs/managing-content/

title: Fitting Censored and Truncated Regression Data Using the Mixture of Experts
  Models
subtitle: ''
summary: ''
authors:
- Tsz Chai Fung
- Andrei L. Badescu
- X. Sheldon Lin
tags: []
categories: []
date: '2022-01-01'
lastmod: 2022-05-27T21:48:51-04:00
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2022-06-01T22:16:32.007893Z'
publication_types:
- '2'
abstract: 'The logit-weighted reduced mixture of experts model (LRMoE) is a flexible
  yet analytically tractable non-linear regression model. Though it has shown usefulness
  in modeling insurance loss frequencies and severities, model calibration becomes
  challenging when censored and truncated data are involved, which is common in actuarial
  practice. In this article, we present an extended expectationâ€“conditional maximization
  (ECM) algorithm that efficiently fits the LRMoE to random censored and random truncated
  regression data. The effectiveness of the proposed algorithm is empirically examined
  through a simulation study. Using real automobile insurance data sets, the usefulness
  and importance of the proposed algorithm are demonstrated through two actuarial
  applications: individual claim reserving and deductible ratemaking. '
publication: '*North American Actuarial Journal*'
doi: 10.1080/10920277.2021.2013896
---
