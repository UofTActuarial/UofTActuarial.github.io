@article{tseung_badescu_fung_lin_2021, 
	title={LRMoE.jl: a software package for insurance loss modelling using mixture of experts regression model}, 
	volume={15}, 
	DOI={10.1017/S1748499521000087}, 
	number={2}, 
	journal={Annals of Actuarial Science}, 
	publisher={Cambridge University Press}, 
	author={Tseung, Spark C. and Badescu, Andrei L. and Fung, Tsz Chai and Lin, X. Sheldon}, 
	year={2020}, 
	pages={419–440},
	abstract={This paper introduces a new julia package, LRMoE, a statistical software tailor-made for actuarial applications, which allows actuarial researchers and practitioners to model and analyse insurance loss frequencies and severities using the Logit-weighted Reduced Mixture-of-Experts (LRMoE) model. LRMoE offers several new distinctive features which are motivated by various actuarial applications and mostly cannot be achieved using existing packages for mixture models. Key features include a wider coverage on frequency and severity distributions and their zero inflation, the flexibility to vary classes of distributions across components, parameter estimation under data censoring and truncation and a collection of insurance ratemaking and reserving functions. The package also provides several model evaluation and visualisation functions to help users easily analyse the performance of the fitted model and interpret the model in insurance contexts.}
}

@article{fung_badescu_lin_2019, 
	title={A class of mixture of experts models for general insurance: application to correlated claim frequencies}, 
	volume={49}, 
	DOI={10.1017/asb.2019.25}, 
	number={3}, 
	journal={ASTIN Bulletin}, 
	publisher={Cambridge University Press}, 
	author={Chai Fung, Tsz and Badescu, Andrei L. and Sheldon Lin, X.}, 
	year={2019}, 
	pages={647–688},
	abstract={This paper focuses on the estimation and application aspects of the Erlang count logit-weighted reduced mixture of experts model (EC-LRMoE), which is a fully flexible multivariate insurance claim frequency regression model. We first prove the identifiability property of the proposed model to ensure that it is a suitable candidate for statistical inference. An expectation conditional maximization (ECM) algorithm is developed for efficient model calibrations. Three simulation studies are performed to examine the effectiveness of the proposed ECM algorithm and the versatility of the proposed model. The applicability of the EC-LRMoE is shown through fitting an European automobile insurance data set. Since the data set contains several complex features, we find it necessary to adopt such a flexible model. Apart from showing excellent fitting results, we are able to interpret the fitted model in an insurance perspective and to visualize the relationship between policyholders’ information and their risk level. Finally, we demonstrate how the fitted model may be useful for insurance ratemaking.}
}

@article{fung_badescu_lin_2022,
	author = {Tsz Chai Fung and Andrei L. Badescu and X. Sheldon Lin},
	title = {Fitting Censored and Truncated Regression Data Using the Mixture of Experts Models},
	journal = {North American Actuarial Journal},
	volume = {0},
	number = {0},
	pages = {1-25},
	year  = {2022},
	publisher = {Routledge},
	doi = {10.1080/10920277.2021.2013896},
	eprint = {https://doi.org/10.1080/10920277.2021.2013896},
	abstract = {The logit-weighted reduced mixture of experts model (LRMoE) is a flexible yet analytically tractable non-linear regression model. Though it has shown usefulness in modeling insurance loss frequencies and severities, model calibration becomes challenging when censored and truncated data are involved, which is common in actuarial practice. In this article, we present an extended expectation–conditional maximization (ECM) algorithm that efficiently fits the LRMoE to random censored and random truncated regression data. The effectiveness of the proposed algorithm is empirically examined through a simulation study. Using real automobile insurance data sets, the usefulness and importance of the proposed algorithm are demonstrated through two actuarial applications: individual claim reserving and deductible ratemaking. }
}

@article{gui_huang_lin_2021,
	title = {Fitting multivariate Erlang mixtures to data: A roughness penalty approach},
	journal = {Journal of Computational and Applied Mathematics},
	volume = {386},
	pages = {113216},
	year = {2021},
	issn = {0377-0427},
	doi = {https://doi.org/10.1016/j.cam.2020.113216},
	author = {Wenyong Gui and Rongtan Huang and X. Sheldon Lin},
	keywords = {Multivariate Erlang mixtures, Truncated and censored data, GECM algorithm, Roughness penalty},
	abstract = {The class of multivariate Erlang mixtures with common scale parameter has many desirable properties and has widely been used in insurance loss modeling. The parameters of a multivariate Erlang mixture are normally estimated using an expectation–maximization (EM) algorithm as shown in Lee and Lin (2012) and Verbelen et al. (2016). However, when fitting the mixture to data of high dimension, the fitted density surface is often not smooth (with deep peaks and valleys) and the tail fitting may also be rather unsatisfactory. In this paper, we propose a generalized expectation conditional maximization (GECM) algorithm that maximizes a penalized likelihood with a proposed roughness penalty. The roughness penalty is based on integrated squared second derivative of the density function of aggregate data, which is used in functional data analysis. We illustrate the performance of the proposed method through some numerical experiments and real data applications.}
}

@article{fung_badescu_lin_2021,
	author = {Tsz Chai Fung and Andrei L. Badescu and X. Sheldon Lin},
	title = {A New Class of Severity Regression Models with an Application to IBNR Prediction},
	journal = {North American Actuarial Journal},
	volume = {25},
	number = {2},
	pages = {206-231},
	year  = {2021},
	publisher = {Routledge},
	doi = {10.1080/10920277.2020.1729813},
	abstract = {Insurance loss severity data often exhibit heavy-tailed behavior, complex distributional characteristics such as multimodality, and peculiar links between policyholders’ risk profiles and claim amounts. To capture these features, we propose a transformed Gamma logit-weighted mixture of experts (TG-LRMoE) model for severity regression. The model possesses several desirable properties. The TG-LRMoE satisfies the denseness property that warrants its full versatility in capturing any distribution and regression structures. It may effectively extrapolate a wide range of tail behavior. The model is also identifiable, which further ensures its suitability for statistical inference. To make the TG-LRMoE computationally tractable, an expectation conditional maximization (ECM) algorithm with parameter penalization is developed for efficient and robust parameter estimation. The proposed model is applied to fit the severity and reporting delay components of a European automobile insurance dataset. In addition to obtaining excellent goodness of fit, the proposed model is shown to be useful and crucial for adequate prediction of incurred but not reported (IBNR) reserves through out-of-sample testing. }
}

@article{lin_yang_2020, 
	title={Efficient dynamic hedging for large variable annuity portfolios with multiple underlying assets}, 
	volume={50}, 
	DOI={10.1017/asb.2020.26}, 
	number={3}, 
	journal={ASTIN Bulletin}, 
	publisher={Cambridge University Press}, 
	author={Lin, X. Sheldon and Yang, Shuai}, 
	year={2020}, 
	pages={913–957},
	abstract={A variable annuity (VA) is an equity-linked annuity that provides investment guarantees to its policyholder and its contributions are normally invested in multiple underlying assets (e.g., mutual funds), which exposes VA liability to significant market risks. Hedging the market risks is therefore crucial in risk managing a VA portfolio as the VA guarantees are long-dated liability that may span over decades. In order to hedge the VA liability, the issuing insurance company would need to construct a hedging portfolio consisting of the underlying assets whose positions are often determined by the liability Greeks such as partial dollar Deltas. Usually, these quantities are calculated via nested simulation. For insurance companies that manage large VA portfolios (e.g.,100K+ policies), calculating those quantities is extremely time-consuming or even prohibitive due to the complexity of the guarantee payoffs and the stochastic-on-stochastic nature of the nested simulation algorithm. In this paper, we extend the surrogate model-assisted nest simulation approach in Lin &Yang (2020) to efficiently calculate the total VA liability and the partial dollar Deltas for large VA portfolios with multiple underlying assets. In our proposed algorithm, the nested simulation is run using small sets of selected representative policies and representative outer-loops. As a result, the computing time is substantially reduced. The computational advantage of the proposed algorithm and the importance of dynamic hedging are further illustrated through a profit and loss (P&L) analysis for a large synthetic VA portfolio. Moreover, the robustness of the performance of the proposed algorithm is tested with multiple simulation runs. Numerical results show that the proposed algorithm is able to accurately approximate different quantities of interest and the performance is robust with respect to different sets of parameter inputs.}
}

@article{lin_yang_2020b,
	title = {Fast and efficient nested simulation for large variable annuity portfolios: A surrogate modeling approach},
	journal = {Insurance: Mathematics and Economics},
	volume = {91},
	pages = {85-103},
	year = {2020},
	issn = {0167-6687},
	doi = {https://doi.org/10.1016/j.insmatheco.2020.01.002},
	author = {X. Sheldon Lin and Shuai Yang},
	keywords = {Variable annuity portfolio, Nested-simulation, Surrogate modeling, Spline regression, Population sampling},
	abstract = {The nested-simulation is commonly used for calculating the predictive distribution of the total variable annuity (VA) liabilities of large VA portfolios. Due to the large numbers of policies, inner-loops and outer-loops, running the nested-simulation for a large VA portfolio is extremely time consuming and often prohibitive. In this paper, the use of surrogate models is incorporated into the nested-simulation algorithm so that the relationship between the inputs and the outputs of a simulation model is approximated by various statistical models. As a result, the nested-simulation algorithm can be run with much smaller numbers of different inputs. Specifically, a spline regression model is used to reduce the number of outer-loops and a model-assisted finite population estimation framework is adapted to reduce the number of policies in use for the nested-simulation. From simulation studies, our proposed algorithm is able to accurately approximate the predictive distribution of the total VA liability at a significantly reduced running time.}
}

@article{fung_badescu_lin_2019b,
	title = {A class of mixture of experts models for general insurance: Theoretical developments},
	journal = {Insurance: Mathematics and Economics},
	volume = {89},
	pages = {111-127},
	year = {2019},
	issn = {0167-6687},
	doi = {https://doi.org/10.1016/j.insmatheco.2019.09.007},
	author = {Tsz Chai Fung and Andrei L. Badescu and X. Sheldon Lin},
	keywords = {Claim frequency and severity modeling, Denseness theory, Mixture of experts models, Multivariate regression analysis, Neural network},
	abstract = {In the Property and Casualty (P&C) ratemaking process, it is critical to understand the effect of policyholders’ risk profile to the number and amount of claims, the dependence among various business lines and the claim distributions. To include all the above features, it is essential to develop a regression model which is flexible and theoretically justified. Motivated by the issues above, we propose a class of logit-weighted reduced mixture of experts (LRMoE) models for multivariate claim frequencies or severities distributions. LRMoE is interpretable, as it has two components: Gating functions, which classify policyholders into various latent sub-classes; and Expert functions, which govern the distributional properties of the claims. Also, upon the development of denseness theory in regression setting, we can heuristically interpret the LRMoE as a “fully flexible” model to capture any distributional, dependence and regression structures subject to a denseness condition. Further, the mathematical tractability of the LRMoE is guaranteed since it satisfies various marginalization and moment properties. Finally, we discuss some special choices of expert functions that make the corresponding LRMoE “fully flexible”. In the subsequent paper (Fung et al., 2019b), we will focus on the estimation and application aspects of the LRMoE.}
}

@article{badescu_chen_lin_tang_2019, 
	title={A marked Cox model for the number of IBNR claims: estimation and application}, 
	volume={49}, 
	DOI={10.1017/asb.2019.15}, 
	number={3}, 
	journal={ASTIN Bulletin}, 
	publisher={Cambridge University Press}, 
	author={Badescu, Andrei L. and Chen, Tianle and Lin, X. Sheldon and Tang, Dameng}, 
	year={2019}, 
	pages={709–739},
	abstract={Incurred but not reported (IBNR) loss reserving is of great importance for Property & Casualty (P&C) insurers. However, the temporal dependence exhibited in the claim arrival process is not reflected in many current loss reserving models, which might affect the accuracy of the IBNR reserve predictions. To overcome this shortcoming, we proposed a marked Cox process and showed its many desirable properties in Badescu et al. (2016). In this paper, we consider the model estimation and applications. We first present an expectation–maximization (EM) algorithm which guarantees the efficiency of the estimators unlike the moment estimation methods widely used in estimating Cox processes. In addition, the proposed fitting algorithm can be implemented at a reasonable computational cost. We examine the performance of the proposed algorithm through simulation studies. The applicability of the proposed model is tested by fitting it to a real insurance claim data set. Through out-of-sample tests, we find that the proposed model can provide realistic predictive distributions.}
}

@article{fung_badescu_lin_2019c,
	author = {Tsz Chai Fung and Andrei L. Badescu and X. Sheldon Lin},
	title = {Multivariate Cox Hidden Markov models with an application to operational risk},
	journal = {Scandinavian Actuarial Journal},
	volume = {2019},
	number = {8},
	pages = {686-710},
	year  = {2019},
	publisher = {Taylor & Francis},
	doi = {10.1080/03461238.2019.1598482},
	abstract = {Modeling multivariate time-series aggregate losses is an important actuarial topic that is very challenging due to the fact that losses can be serially dependent with heterogeneous dependence structures across loss types and business lines. In this paper, we investigate a flexible class of multivariate Cox Hidden Markov Models for the joint arrival process of loss events. Some of the nice properties possessed by this class of models, such as closed-form expressions, thinning properties and model versatility are discussed in details. We provide the expectation-maximization (EM) algorithm for efficient model calibration. Applying the proposed model to an operational risk dataset, we demonstrate that the model offers sufficient flexibility to capture most characteristics of the observed loss frequencies. By modeling the log-transformed loss severities through mixture of Erlang distributions, we can model the aggregate losses. Finally, out-of-sample testing shows that the proposed model is adequate to predict short-term future operational risk losses. }
}

@article{yin_lin_huang_yuan_2019,
	title = {On the consistency of penalized MLEs for Erlang mixtures},
	journal = {Statistics & Probability Letters},
	volume = {145},
	pages = {12-20},
	year = {2019},
	issn = {0167-7152},
	doi = {https://doi.org/10.1016/j.spl.2018.08.004},
	author = {Cuihong Yin and X. Sheldon Lin and Rongtan Huang and Haili Yuan},
	keywords = {Erlang mixture, Insurance loss modeling, Penalized maximum likelihood estimates, Consistency},
	abstract = {In Yin and Lin (2016), a new penalty, termed as iSCAD penalty, is proposed to obtain the maximum likelihood estimates (MLEs) of the weights and the common scale parameter of an Erlang mixture model. In that paper, it is shown through simulation studies and a real data application that the penalty provides an efficient way to determine the MLEs and the order of the mixture. In this paper, we provide a theoretical justification and show that the penalized maximum likelihood estimators of the weights and the scale parameter as well as the order of mixture are all consistent.}
}

@article{gui_huang_lin_2018,
	title = {Fitting the Erlang mixture model to data via a GEM-CMM algorithm},
	journal = {Journal of Computational and Applied Mathematics},
	volume = {343},
	pages = {189-205},
	year = {2018},
	issn = {0377-0427},
	doi = {https://doi.org/10.1016/j.cam.2018.04.032},
	author = {Wenyong Gui and Rongtan Huang and X. Sheldon Lin},
	keywords = {Erlang mixture model, Insurance loss data, Generalized EM algorithm, Clusterized method of moments, Local search method},
	abstract = {The Erlang mixture model with common scale parameter is flexible and analytically tractable. As such, it is a useful model to fit insurance loss data and to calculate quantities of interest for insurance risk management. In this paper, we propose a generalized expectation–maximization (GEM) algorithm along with a clusterized method of moments (CMM) to estimate the model parameters. The GEM algorithm not only estimates the mixing weights and scale parameter of the model but also estimates the shape parameters of the model using a local search method. The CMM method enables to produce quality initial estimates for the GEM algorithm. As a result, the proposed approach provides an efficient algorithm that can fit the model to the body and the tail of truncated and censored loss data well and converges fast. We examine the performance of the proposed approach through several simulation studies and apply it to fit the Erlang mixture model to two real loss data sets.}
}

@article{lee_lin_2018,
	author = {Simon C. K. Lee and Sheldon Lin},
	title = {Delta Boosting Machine with Application to General Insurance},
	journal = {North American Actuarial Journal},
	volume = {22},
	number = {3},
	pages = {405-425},
	year  = {2018},
	publisher = {Routledge},
	doi = {10.1080/10920277.2018.1431131},
	abstract = {In this article, we introduce Delta Boosting (DB) as a new member of the boosting family. Similar to the popular Gradient Boosting (GB), this new member is presented as a forward stagewise additive model that attempts to reduce the loss at each iteration by sequentially fitting a simple base learner to complement the running predictions. Instead of relying on the negative gradient, as is the case for GB, DB adopts a new measure called delta as the basis. Delta is defined as the loss minimizer at an observation level. We also show that DB is the optimal boosting member for a wide range of loss functions. The optimality is a consequence of DB solving for the split and adjustment simultaneously to maximize loss reduction at each iteration. In addition, we introduce an asymptotic version of DB that works well for all twice-differentiable strictly convex loss functions. This asymptotic behavior does not depend on the number of observations, but rather on a high number of iterations that can be augmented through common regularization techniques. We show that the basis in the asymptotic extension differs from the basis in GB only by a multiple of the second derivative of the log-likelihood. The multiple is considered to be a correction factor, one that corrects the bias toward the observations with high second derivatives in GB. When negative log-likelihood is used as the loss function, this correction can be interpreted as a credibility adjustment for the process variance. Simulation studies and real data application we conducted suggest that DB is a significant improvement over GB. The performance of the asymptotic version is less dramatic, but the improvement is still compelling. Like GB, DB provides a high transparency to users, and we can review the marginal influence of variables through relative importance charts and the partial dependence plots. We can also assess the overall model performance through evaluating the losses, lifts, and double lifts on the holdout sample. }
}

@article{yin_lin_2016, 
	title={Efficient estimation of Erlang mixtures using iSCAD penalty with insurance application}, 
	volume={46}, 
	DOI={10.1017/asb.2016.14}, 
	number={3}, 
	journal={ASTIN Bulletin}, 
	publisher={Cambridge University Press},
	author={Yin, Cuihong and Lin, X. Sheldon}, 
	year={2016}, 
	pages={779–799},
	abstract={The Erlang mixture model has been widely used in modeling insurance losses due to its desirable distributional properties. In this paper, we consider the problem of efficient estimation of the Erlang mixture model. We present a new thresholding penalty function and a corresponding EM algorithm to estimate model parameters and to determine the order of the mixture. Using simulation studies and a real data application, we demonstrate the efficiency of the EM algorithm.}
 }

 @article{badescu_lin_tang_2016,
	title = {A marked Cox model for the number of IBNR claims: Theory},
	journal = {Insurance: Mathematics and Economics},
	volume = {69},
	pages = {29-37},
	year = {2016},
	issn = {0167-6687},
	doi = {https://doi.org/10.1016/j.insmatheco.2016.03.016},
	author = {Andrei L. Badescu and X. Sheldon Lin and Dameng Tang},
	keywords = {IBNR claims, Loss reserving, Cox model, Hidden Markov chain, Temporal dependence, Pascal mixture},
	abstract = {Incurred but not reported (IBNR) loss reserving is an important issue for Property & Casualty (P&C) insurers. To calculate IBNR reserve, one needs to model claim arrivals and then predict IBNR claims. However, factors such as temporal dependence among claim arrivals and environmental variation are often not incorporated in many of the current loss reserving models, which may greatly affect the accuracy of IBNR predictions. In this paper, we propose to model the claim arrival process together with its reporting delays as a marked Cox process. Our model is versatile in modeling temporal dependence, allowing also for natural interpretations. This paper focuses mainly on the theoretical aspects of the proposed model. We show that the associated reported claim process and IBNR claim process are both marked Cox processes with easily convertible intensity functions and marking distributions. The proposed model can also account for fluctuations in the exposure. By an order statistics property, we show that the corresponding discretely observed process preserves all the information about the claim arrivals. Finally, we derive closed-form expressions for both the autocorrelation function (ACF) and the distributions of the numbers of reported claims and IBNR claims. Model estimation and its applications are considered in a subsequent paper, Badescu et al. (2015b).}
}

@article{verbelen_gong_antonio_badescu_lin_2015, 
	title={Fitting mixtures of Erlangs to censored and truncated data using the EM algorithm}, 
	volume={45}, 
	DOI={10.1017/asb.2015.15}, 
	number={3}, 
	journal={ASTIN Bulletin}, 
	publisher={Cambridge University Press}, 
	author={Verbelen, Roel and Gong, Lan and Antonio, Katrien and Badescu, Andrei and Lin, Sheldon}, 
	year={2015}, 
	pages={729–758},
	abstract={We discuss how to fit mixtures of Erlangs to censored and truncated data by iteratively using the EM algorithm. Mixtures of Erlangs form a very versatile, yet analytically tractable, class of distributions making them suitable for loss modeling purposes. The effectiveness of the proposed algorithm is demonstrated on simulated data as well as real data sets.}
}

@article{pesenti_millossovich_tsanakas_2019,
	title = {Reverse sensitivity testing: What does it take to break the model?},
	journal = {European Journal of Operational Research},
	volume = {274},
	number = {2},
	pages = {654-670},
	year = {2019},
	issn = {0377-2217},
	doi = {https://doi.org/10.1016/j.ejor.2018.10.003},
	author = {Silvana M. Pesenti and Pietro Millossovich and Andreas Tsanakas},
	keywords = {Robustness and sensitivity analysis, Risk management, Value-at-Risk, Expected Shortfall, Stress testing},
	abstract = {Sensitivity analysis is an important component of model building, interpretation and validation. A model comprises a vector of random input factors, an aggregation function mapping input factors to a random output, and a (baseline) probability measure. A risk measure, such as Value-at-Risk and Expected Shortfall, maps the distribution of the output to the real line. As is common in risk management, the value of the risk measure applied to the output is a decision variable. Therefore, it is of interest to associate a critical increase in the risk measure to specific input factors. We propose a global and model-independent framework, termed ‘reverse sensitivity testing’, comprising three steps: (a) an output stress is specified, corresponding to an increase in the risk measure(s); (b) a (stressed) probability measure is derived, minimising the Kullback–Leibler divergence with respect to the baseline probability, under constraints generated by the output stress; (c) changes in the distributions of input factors are evaluated. We argue that a substantial change in the distribution of an input factor corresponds to high sensitivity to that input and introduce a novel sensitivity measure to formalise this insight. Implementation of reverse sensitivity testing in a Monte Carlo setting can be performed on a single set of input/output scenarios, simulated under the baseline model. Thus the approach circumvents the need for additional computationally expensive evaluations of the aggregation function. We illustrate the proposed approach through numerical examples with a simple insurance portfolio and a model of a London Insurance Market portfolio used in industry.}
}

@article{pesenti_2021,
  title={Reverse Sensitivity Analysis for Risk Modelling},
  author={Pesenti, Silvana M},
  journal={Available at SSRN 3878879},
  year={2021},
  url={https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3878879},
  abstract = {We consider the problem where a modeller conducts sensitivity analysis of a model consisting of random input factors, a corresponding random output of interest, and a baseline probability measure. The modeller seeks to understand how the model (the distribution of the input factors as well as the output) changes under a stress on the output's distribution. Specifically, for a stress on the output random variable, we derive the unique stressed distribution of the output that is closest in the Wasserstein distance to the baseline output's distribution and satisfies the stress. We further derive the stressed model, including the stressed distribution of the inputs, which can be calculated in a numerically efficient way from a set of baseline Monte Carlo samples and which is implemented in the R package SWIM on CRAN. The proposed reverse sensitivity analysis framework is model-free and allows for stresses on the output such as (a) the mean and variance, (b) any distortion risk measure including the Value-at-Risk and Expected-Shortfall, and (c) expected utility type constraints, thus making the reverse sensitivity analysis framework suitable for risk models.}
}
