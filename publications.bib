@article{tseung_badescu_fung_lin_2021, 
	title={LRMoE.jl: a software package for insurance loss modelling using mixture of experts regression model}, 
	volume={15}, 
	DOI={10.1017/S1748499521000087}, 
	number={2}, 
	journal={Annals of Actuarial Science}, 
	publisher={Cambridge University Press}, 
	author={Tseung, Spark C. and Badescu, Andrei L. and Fung, Tsz Chai and Lin, X. Sheldon}, 
	year={2021}, 
	pages={419–440},
	abstract={This paper introduces a new julia package, LRMoE, a statistical software tailor-made for actuarial applications, which allows actuarial researchers and practitioners to model and analyse insurance loss frequencies and severities using the Logit-weighted Reduced Mixture-of-Experts (LRMoE) model. LRMoE offers several new distinctive features which are motivated by various actuarial applications and mostly cannot be achieved using existing packages for mixture models. Key features include a wider coverage on frequency and severity distributions and their zero inflation, the flexibility to vary classes of distributions across components, parameter estimation under data censoring and truncation and a collection of insurance ratemaking and reserving functions. The package also provides several model evaluation and visualisation functions to help users easily analyse the performance of the fitted model and interpret the model in insurance contexts.}
}

@article{fung_badescu_lin_2019, 
	title={A class of mixture of experts models for general insurance: application to correlated claim frequencies}, 
	volume={49}, 
	DOI={10.1017/asb.2019.25}, 
	number={3}, 
	journal={ASTIN Bulletin}, 
	publisher={Cambridge University Press}, 
	author={Chai Fung, Tsz and Badescu, Andrei L. and Sheldon Lin, X.}, 
	year={2019}, 
	pages={647–688},
	abstract={This paper focuses on the estimation and application aspects of the Erlang count logit-weighted reduced mixture of experts model (EC-LRMoE), which is a fully flexible multivariate insurance claim frequency regression model. We first prove the identifiability property of the proposed model to ensure that it is a suitable candidate for statistical inference. An expectation conditional maximization (ECM) algorithm is developed for efficient model calibrations. Three simulation studies are performed to examine the effectiveness of the proposed ECM algorithm and the versatility of the proposed model. The applicability of the EC-LRMoE is shown through fitting an European automobile insurance data set. Since the data set contains several complex features, we find it necessary to adopt such a flexible model. Apart from showing excellent fitting results, we are able to interpret the fitted model in an insurance perspective and to visualize the relationship between policyholders’ information and their risk level. Finally, we demonstrate how the fitted model may be useful for insurance ratemaking.}
}

@article{fung_badescu_lin_2022,
	author = {Tsz Chai Fung and Andrei L. Badescu and X. Sheldon Lin},
	title = {Fitting Censored and Truncated Regression Data Using the Mixture of Experts Models},
	journal = {North American Actuarial Journal},
	volume = {0},
	number = {0},
	pages = {1-25},
	year  = {2022},
	publisher = {Routledge},
	doi = {10.1080/10920277.2021.2013896},
	eprint = {https://doi.org/10.1080/10920277.2021.2013896},
	abstract = {The logit-weighted reduced mixture of experts model (LRMoE) is a flexible yet analytically tractable non-linear regression model. Though it has shown usefulness in modeling insurance loss frequencies and severities, model calibration becomes challenging when censored and truncated data are involved, which is common in actuarial practice. In this article, we present an extended expectation–conditional maximization (ECM) algorithm that efficiently fits the LRMoE to random censored and random truncated regression data. The effectiveness of the proposed algorithm is empirically examined through a simulation study. Using real automobile insurance data sets, the usefulness and importance of the proposed algorithm are demonstrated through two actuarial applications: individual claim reserving and deductible ratemaking. }
}

@article{gui_huang_lin_2021,
	title = {Fitting multivariate Erlang mixtures to data: A roughness penalty approach},
	journal = {Journal of Computational and Applied Mathematics},
	volume = {386},
	pages = {113216},
	year = {2021},
	issn = {0377-0427},
	doi = {https://doi.org/10.1016/j.cam.2020.113216},
	author = {Wenyong Gui and Rongtan Huang and X. Sheldon Lin},
	keywords = {Multivariate Erlang mixtures, Truncated and censored data, GECM algorithm, Roughness penalty},
	abstract = {The class of multivariate Erlang mixtures with common scale parameter has many desirable properties and has widely been used in insurance loss modeling. The parameters of a multivariate Erlang mixture are normally estimated using an expectation–maximization (EM) algorithm as shown in Lee and Lin (2012) and Verbelen et al. (2016). However, when fitting the mixture to data of high dimension, the fitted density surface is often not smooth (with deep peaks and valleys) and the tail fitting may also be rather unsatisfactory. In this paper, we propose a generalized expectation conditional maximization (GECM) algorithm that maximizes a penalized likelihood with a proposed roughness penalty. The roughness penalty is based on integrated squared second derivative of the density function of aggregate data, which is used in functional data analysis. We illustrate the performance of the proposed method through some numerical experiments and real data applications.}
}

@article{fung_badescu_lin_2021,
	author = {Tsz Chai Fung and Andrei L. Badescu and X. Sheldon Lin},
	title = {A New Class of Severity Regression Models with an Application to IBNR Prediction},
	journal = {North American Actuarial Journal},
	volume = {25},
	number = {2},
	pages = {206-231},
	year  = {2021},
	publisher = {Routledge},
	doi = {10.1080/10920277.2020.1729813},
	abstract = {Insurance loss severity data often exhibit heavy-tailed behavior, complex distributional characteristics such as multimodality, and peculiar links between policyholders’ risk profiles and claim amounts. To capture these features, we propose a transformed Gamma logit-weighted mixture of experts (TG-LRMoE) model for severity regression. The model possesses several desirable properties. The TG-LRMoE satisfies the denseness property that warrants its full versatility in capturing any distribution and regression structures. It may effectively extrapolate a wide range of tail behavior. The model is also identifiable, which further ensures its suitability for statistical inference. To make the TG-LRMoE computationally tractable, an expectation conditional maximization (ECM) algorithm with parameter penalization is developed for efficient and robust parameter estimation. The proposed model is applied to fit the severity and reporting delay components of a European automobile insurance dataset. In addition to obtaining excellent goodness of fit, the proposed model is shown to be useful and crucial for adequate prediction of incurred but not reported (IBNR) reserves through out-of-sample testing. }
}

@article{lin_yang_2020, 
	title={Efficient dynamic hedging for large variable annuity portfolios with multiple underlying assets}, 
	volume={50}, 
	DOI={10.1017/asb.2020.26}, 
	number={3}, 
	journal={ASTIN Bulletin}, 
	publisher={Cambridge University Press}, 
	author={Lin, X. Sheldon and Yang, Shuai}, 
	year={2020}, 
	pages={913–957},
	abstract={A variable annuity (VA) is an equity-linked annuity that provides investment guarantees to its policyholder and its contributions are normally invested in multiple underlying assets (e.g., mutual funds), which exposes VA liability to significant market risks. Hedging the market risks is therefore crucial in risk managing a VA portfolio as the VA guarantees are long-dated liability that may span over decades. In order to hedge the VA liability, the issuing insurance company would need to construct a hedging portfolio consisting of the underlying assets whose positions are often determined by the liability Greeks such as partial dollar Deltas. Usually, these quantities are calculated via nested simulation. For insurance companies that manage large VA portfolios (e.g.,100K+ policies), calculating those quantities is extremely time-consuming or even prohibitive due to the complexity of the guarantee payoffs and the stochastic-on-stochastic nature of the nested simulation algorithm. In this paper, we extend the surrogate model-assisted nest simulation approach in Lin &Yang (2020) to efficiently calculate the total VA liability and the partial dollar Deltas for large VA portfolios with multiple underlying assets. In our proposed algorithm, the nested simulation is run using small sets of selected representative policies and representative outer-loops. As a result, the computing time is substantially reduced. The computational advantage of the proposed algorithm and the importance of dynamic hedging are further illustrated through a profit and loss (P&L) analysis for a large synthetic VA portfolio. Moreover, the robustness of the performance of the proposed algorithm is tested with multiple simulation runs. Numerical results show that the proposed algorithm is able to accurately approximate different quantities of interest and the performance is robust with respect to different sets of parameter inputs.}
}

@article{lin_yang_2020b,
	title = {Fast and efficient nested simulation for large variable annuity portfolios: A surrogate modeling approach},
	journal = {Insurance: Mathematics and Economics},
	volume = {91},
	pages = {85-103},
	year = {2020},
	issn = {0167-6687},
	doi = {https://doi.org/10.1016/j.insmatheco.2020.01.002},
	author = {X. Sheldon Lin and Shuai Yang},
	keywords = {Variable annuity portfolio, Nested-simulation, Surrogate modeling, Spline regression, Population sampling},
	abstract = {The nested-simulation is commonly used for calculating the predictive distribution of the total variable annuity (VA) liabilities of large VA portfolios. Due to the large numbers of policies, inner-loops and outer-loops, running the nested-simulation for a large VA portfolio is extremely time consuming and often prohibitive. In this paper, the use of surrogate models is incorporated into the nested-simulation algorithm so that the relationship between the inputs and the outputs of a simulation model is approximated by various statistical models. As a result, the nested-simulation algorithm can be run with much smaller numbers of different inputs. Specifically, a spline regression model is used to reduce the number of outer-loops and a model-assisted finite population estimation framework is adapted to reduce the number of policies in use for the nested-simulation. From simulation studies, our proposed algorithm is able to accurately approximate the predictive distribution of the total VA liability at a significantly reduced running time.}
}

@article{fung_badescu_lin_2019b,
	title = {A class of mixture of experts models for general insurance: Theoretical developments},
	journal = {Insurance: Mathematics and Economics},
	volume = {89},
	pages = {111-127},
	year = {2019},
	issn = {0167-6687},
	doi = {https://doi.org/10.1016/j.insmatheco.2019.09.007},
	author = {Tsz Chai Fung and Andrei L. Badescu and X. Sheldon Lin},
	keywords = {Claim frequency and severity modeling, Denseness theory, Mixture of experts models, Multivariate regression analysis, Neural network},
	abstract = {In the Property and Casualty (P&C) ratemaking process, it is critical to understand the effect of policyholders’ risk profile to the number and amount of claims, the dependence among various business lines and the claim distributions. To include all the above features, it is essential to develop a regression model which is flexible and theoretically justified. Motivated by the issues above, we propose a class of logit-weighted reduced mixture of experts (LRMoE) models for multivariate claim frequencies or severities distributions. LRMoE is interpretable, as it has two components: Gating functions, which classify policyholders into various latent sub-classes; and Expert functions, which govern the distributional properties of the claims. Also, upon the development of denseness theory in regression setting, we can heuristically interpret the LRMoE as a “fully flexible” model to capture any distributional, dependence and regression structures subject to a denseness condition. Further, the mathematical tractability of the LRMoE is guaranteed since it satisfies various marginalization and moment properties. Finally, we discuss some special choices of expert functions that make the corresponding LRMoE “fully flexible”. In the subsequent paper (Fung et al., 2019b), we will focus on the estimation and application aspects of the LRMoE.}
}

@article{badescu_chen_lin_tang_2019, 
	title={A marked Cox model for the number of IBNR claims: estimation and application}, 
	volume={49}, 
	DOI={10.1017/asb.2019.15}, 
	number={3}, 
	journal={ASTIN Bulletin}, 
	publisher={Cambridge University Press}, 
	author={Badescu, Andrei L. and Chen, Tianle and Lin, X. Sheldon and Tang, Dameng}, 
	year={2019}, 
	pages={709–739},
	abstract={Incurred but not reported (IBNR) loss reserving is of great importance for Property & Casualty (P&C) insurers. However, the temporal dependence exhibited in the claim arrival process is not reflected in many current loss reserving models, which might affect the accuracy of the IBNR reserve predictions. To overcome this shortcoming, we proposed a marked Cox process and showed its many desirable properties in Badescu et al. (2016). In this paper, we consider the model estimation and applications. We first present an expectation–maximization (EM) algorithm which guarantees the efficiency of the estimators unlike the moment estimation methods widely used in estimating Cox processes. In addition, the proposed fitting algorithm can be implemented at a reasonable computational cost. We examine the performance of the proposed algorithm through simulation studies. The applicability of the proposed model is tested by fitting it to a real insurance claim data set. Through out-of-sample tests, we find that the proposed model can provide realistic predictive distributions.}
}

@article{fung_badescu_lin_2019c,
	author = {Tsz Chai Fung and Andrei L. Badescu and X. Sheldon Lin},
	title = {Multivariate Cox Hidden Markov models with an application to operational risk},
	journal = {Scandinavian Actuarial Journal},
	volume = {2019},
	number = {8},
	pages = {686-710},
	year  = {2019},
	publisher = {Taylor & Francis},
	doi = {10.1080/03461238.2019.1598482},
	abstract = {Modeling multivariate time-series aggregate losses is an important actuarial topic that is very challenging due to the fact that losses can be serially dependent with heterogeneous dependence structures across loss types and business lines. In this paper, we investigate a flexible class of multivariate Cox Hidden Markov Models for the joint arrival process of loss events. Some of the nice properties possessed by this class of models, such as closed-form expressions, thinning properties and model versatility are discussed in details. We provide the expectation-maximization (EM) algorithm for efficient model calibration. Applying the proposed model to an operational risk dataset, we demonstrate that the model offers sufficient flexibility to capture most characteristics of the observed loss frequencies. By modeling the log-transformed loss severities through mixture of Erlang distributions, we can model the aggregate losses. Finally, out-of-sample testing shows that the proposed model is adequate to predict short-term future operational risk losses. }
}

@article{yin_lin_huang_yuan_2019,
	title = {On the consistency of penalized MLEs for Erlang mixtures},
	journal = {Statistics & Probability Letters},
	volume = {145},
	pages = {12-20},
	year = {2019},
	issn = {0167-7152},
	doi = {https://doi.org/10.1016/j.spl.2018.08.004},
	author = {Cuihong Yin and X. Sheldon Lin and Rongtan Huang and Haili Yuan},
	keywords = {Erlang mixture, Insurance loss modeling, Penalized maximum likelihood estimates, Consistency},
	abstract = {In Yin and Lin (2016), a new penalty, termed as iSCAD penalty, is proposed to obtain the maximum likelihood estimates (MLEs) of the weights and the common scale parameter of an Erlang mixture model. In that paper, it is shown through simulation studies and a real data application that the penalty provides an efficient way to determine the MLEs and the order of the mixture. In this paper, we provide a theoretical justification and show that the penalized maximum likelihood estimators of the weights and the scale parameter as well as the order of mixture are all consistent.}
}

@article{gui_huang_lin_2018,
	title = {Fitting the Erlang mixture model to data via a GEM-CMM algorithm},
	journal = {Journal of Computational and Applied Mathematics},
	volume = {343},
	pages = {189-205},
	year = {2018},
	issn = {0377-0427},
	doi = {https://doi.org/10.1016/j.cam.2018.04.032},
	author = {Wenyong Gui and Rongtan Huang and X. Sheldon Lin},
	keywords = {Erlang mixture model, Insurance loss data, Generalized EM algorithm, Clusterized method of moments, Local search method},
	abstract = {The Erlang mixture model with common scale parameter is flexible and analytically tractable. As such, it is a useful model to fit insurance loss data and to calculate quantities of interest for insurance risk management. In this paper, we propose a generalized expectation–maximization (GEM) algorithm along with a clusterized method of moments (CMM) to estimate the model parameters. The GEM algorithm not only estimates the mixing weights and scale parameter of the model but also estimates the shape parameters of the model using a local search method. The CMM method enables to produce quality initial estimates for the GEM algorithm. As a result, the proposed approach provides an efficient algorithm that can fit the model to the body and the tail of truncated and censored loss data well and converges fast. We examine the performance of the proposed approach through several simulation studies and apply it to fit the Erlang mixture model to two real loss data sets.}
}

@article{lee_lin_2018,
	author = {Simon C. K. Lee and Sheldon Lin},
	title = {Delta Boosting Machine with Application to General Insurance},
	journal = {North American Actuarial Journal},
	volume = {22},
	number = {3},
	pages = {405-425},
	year  = {2018},
	publisher = {Routledge},
	doi = {10.1080/10920277.2018.1431131},
	abstract = {In this article, we introduce Delta Boosting (DB) as a new member of the boosting family. Similar to the popular Gradient Boosting (GB), this new member is presented as a forward stagewise additive model that attempts to reduce the loss at each iteration by sequentially fitting a simple base learner to complement the running predictions. Instead of relying on the negative gradient, as is the case for GB, DB adopts a new measure called delta as the basis. Delta is defined as the loss minimizer at an observation level. We also show that DB is the optimal boosting member for a wide range of loss functions. The optimality is a consequence of DB solving for the split and adjustment simultaneously to maximize loss reduction at each iteration. In addition, we introduce an asymptotic version of DB that works well for all twice-differentiable strictly convex loss functions. This asymptotic behavior does not depend on the number of observations, but rather on a high number of iterations that can be augmented through common regularization techniques. We show that the basis in the asymptotic extension differs from the basis in GB only by a multiple of the second derivative of the log-likelihood. The multiple is considered to be a correction factor, one that corrects the bias toward the observations with high second derivatives in GB. When negative log-likelihood is used as the loss function, this correction can be interpreted as a credibility adjustment for the process variance. Simulation studies and real data application we conducted suggest that DB is a significant improvement over GB. The performance of the asymptotic version is less dramatic, but the improvement is still compelling. Like GB, DB provides a high transparency to users, and we can review the marginal influence of variables through relative importance charts and the partial dependence plots. We can also assess the overall model performance through evaluating the losses, lifts, and double lifts on the holdout sample. }
}

@article{yin_lin_2016, 
	title={Efficient estimation of Erlang mixtures using iSCAD penalty with insurance application}, 
	volume={46}, 
	DOI={10.1017/asb.2016.14}, 
	number={3}, 
	journal={ASTIN Bulletin}, 
	publisher={Cambridge University Press},
	author={Yin, Cuihong and Lin, X. Sheldon}, 
	year={2016}, 
	pages={779–799},
	abstract={The Erlang mixture model has been widely used in modeling insurance losses due to its desirable distributional properties. In this paper, we consider the problem of efficient estimation of the Erlang mixture model. We present a new thresholding penalty function and a corresponding EM algorithm to estimate model parameters and to determine the order of the mixture. Using simulation studies and a real data application, we demonstrate the efficiency of the EM algorithm.}
 }

 @article{badescu_lin_tang_2016,
	title = {A marked Cox model for the number of IBNR claims: Theory},
	journal = {Insurance: Mathematics and Economics},
	volume = {69},
	pages = {29-37},
	year = {2016},
	issn = {0167-6687},
	doi = {https://doi.org/10.1016/j.insmatheco.2016.03.016},
	author = {Andrei L. Badescu and X. Sheldon Lin and Dameng Tang},
	keywords = {IBNR claims, Loss reserving, Cox model, Hidden Markov chain, Temporal dependence, Pascal mixture},
	abstract = {Incurred but not reported (IBNR) loss reserving is an important issue for Property & Casualty (P&C) insurers. To calculate IBNR reserve, one needs to model claim arrivals and then predict IBNR claims. However, factors such as temporal dependence among claim arrivals and environmental variation are often not incorporated in many of the current loss reserving models, which may greatly affect the accuracy of IBNR predictions. In this paper, we propose to model the claim arrival process together with its reporting delays as a marked Cox process. Our model is versatile in modeling temporal dependence, allowing also for natural interpretations. This paper focuses mainly on the theoretical aspects of the proposed model. We show that the associated reported claim process and IBNR claim process are both marked Cox processes with easily convertible intensity functions and marking distributions. The proposed model can also account for fluctuations in the exposure. By an order statistics property, we show that the corresponding discretely observed process preserves all the information about the claim arrivals. Finally, we derive closed-form expressions for both the autocorrelation function (ACF) and the distributions of the numbers of reported claims and IBNR claims. Model estimation and its applications are considered in a subsequent paper, Badescu et al. (2015b).}
}

@article{verbelen_gong_antonio_badescu_lin_2015, 
	title={Fitting mixtures of Erlangs to censored and truncated data using the EM algorithm}, 
	volume={45}, 
	DOI={10.1017/asb.2015.15}, 
	number={3}, 
	journal={ASTIN Bulletin}, 
	publisher={Cambridge University Press}, 
	author={Verbelen, Roel and Gong, Lan and Antonio, Katrien and Badescu, Andrei and Lin, Sheldon}, 
	year={2015}, 
	pages={729–758},
	abstract={We discuss how to fit mixtures of Erlangs to censored and truncated data by iteratively using the EM algorithm. Mixtures of Erlangs form a very versatile, yet analytically tractable, class of distributions making them suitable for loss modeling purposes. The effectiveness of the proposed algorithm is demonstrated on simulated data as well as real data sets.}
}

@article{pesenti_millossovich_tsanakas_2019,
	title = {Reverse sensitivity testing: What does it take to break the model?},
	journal = {European Journal of Operational Research},
	volume = {274},
	number = {2},
	pages = {654-670},
	year = {2019},
	issn = {0377-2217},
	doi = {https://doi.org/10.1016/j.ejor.2018.10.003},
	author = {Silvana M. Pesenti and Pietro Millossovich and Andreas Tsanakas},
	keywords = {Robustness and sensitivity analysis, Risk management, Value-at-Risk, Expected Shortfall, Stress testing},
	abstract = {Sensitivity analysis is an important component of model building, interpretation and validation. A model comprises a vector of random input factors, an aggregation function mapping input factors to a random output, and a (baseline) probability measure. A risk measure, such as Value-at-Risk and Expected Shortfall, maps the distribution of the output to the real line. As is common in risk management, the value of the risk measure applied to the output is a decision variable. Therefore, it is of interest to associate a critical increase in the risk measure to specific input factors. We propose a global and model-independent framework, termed ‘reverse sensitivity testing’, comprising three steps: (a) an output stress is specified, corresponding to an increase in the risk measure(s); (b) a (stressed) probability measure is derived, minimising the Kullback–Leibler divergence with respect to the baseline probability, under constraints generated by the output stress; (c) changes in the distributions of input factors are evaluated. We argue that a substantial change in the distribution of an input factor corresponds to high sensitivity to that input and introduce a novel sensitivity measure to formalise this insight. Implementation of reverse sensitivity testing in a Monte Carlo setting can be performed on a single set of input/output scenarios, simulated under the baseline model. Thus the approach circumvents the need for additional computationally expensive evaluations of the aggregation function. We illustrate the proposed approach through numerical examples with a simple insurance portfolio and a model of a London Insurance Market portfolio used in industry.}
}

@article{pesenti_2021,
  title={Reverse Sensitivity Analysis for Risk Modelling},
  author={Pesenti, Silvana M},
  journal={Available at SSRN 3878879},
  year={2021},
  url={https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3878879},
  abstract = {We consider the problem where a modeller conducts sensitivity analysis of a model consisting of random input factors, a corresponding random output of interest, and a baseline probability measure. The modeller seeks to understand how the model (the distribution of the input factors as well as the output) changes under a stress on the output's distribution. Specifically, for a stress on the output random variable, we derive the unique stressed distribution of the output that is closest in the Wasserstein distance to the baseline output's distribution and satisfies the stress. We further derive the stressed model, including the stressed distribution of the inputs, which can be calculated in a numerically efficient way from a set of baseline Monte Carlo samples and which is implemented in the R package SWIM on CRAN. The proposed reverse sensitivity analysis framework is model-free and allows for stresses on the output such as (a) the mean and variance, (b) any distortion risk measure including the Value-at-Risk and Expected-Shortfall, and (c) expected utility type constraints, thus making the reverse sensitivity analysis framework suitable for risk models.}
}

@article{pesenti_millossovich_tsanakas_2016,
	author = {Silvana M. Pesenti and Pietro Millossovich and Andreas Tsanakas},
	doi = {doi:10.1515/demo-2016-0020},
	title = {Robustness regions for measures of risk aggregation},
	journal = {Dependence Modeling},
	number = {1},
	volume = {4},
	year = {2016},
	pages = {000010151520160020},
	abstract = {One of risk measures’ key purposes is to consistently rank and distinguish between different risk profiles. From a practical perspective, a risk measure should also be robust, that is, insensitive to small perturbations in input assumptions. It is known in the literature [14, 39], that strong assumptions on the risk measure’s ability to distinguish between risks may lead to a lack of robustness. We address the trade-off between robustness and consistent risk ranking by specifying the regions in the space of distribution functions, where law-invariant convex risk measures are indeed robust. Examples include the set of random variables with bounded second moment and those that are less volatile (in convex order) than random variables in a given uniformly integrable set. Typically, a risk measure is evaluated on the output of an aggregation function defined on a set of random input vectors. Extending the definition of robustness to this setting, we find that law-invariant convex risk measures are robust for any aggregation function that satisfies a linear growth condition in the tail, provided that the set of possible marginals is uniformly integrable. Thus, we obtain that all law-invariant convex risk measures possess the aggregation-robustness property introduced by [26] and further studied by [40]. This is in contrast to the widely-used, non-convex, risk measure Value-at-Risk, whose robustness in a risk aggregation context requires restricting the possible dependence structures of the input vectors.}
}

@article{pesenti_tsanakas_millossovich_2018,
	title = {Euler allocations in the presence of non-linear reinsurance: Comment on Major (2018)},
	journal = {Insurance: Mathematics and Economics},
	volume = {83},
	pages = {29-31},
	year = {2018},
	issn = {0167-6687},
	doi = {https://doi.org/10.1016/j.insmatheco.2018.09.001},
	author = {Silvana M. Pesenti and Andreas Tsanakas and Pietro Millossovich},
	keywords = {Distortion risk measures, Capital allocation, Euler allocation, Aumann–Shapley, Reinsurance, Aggregation},
	abstract = {Major (2018) discusses Euler/Aumann–Shapley allocations for non-linear positively homogeneous portfolios. For such portfolio structures, plausibly arising in the context of reinsurance, he defines a distortion-type risk measure that facilitates assessment of ceded and net losses with reference to gross portfolio outcomes. Subsequently, Major (2018) derives explicit formulas for Euler allocations for this risk measure, thus (sub-)allocating ceded losses to the portfolio’s original components. In this comment, we build on Major’s (2018) insights but take a somewhat different direction, to consider Euler capital allocations for distortion risk measures directly applied to homogeneous portfolios. Explicit formulas are derived and our approach is compared with that of Major (2018) via a numerical example.}
}

@article{pesenti_millossovich_tsanakas_2019,
	title = {Reverse sensitivity testing: What does it take to break the model?},
	journal = {European Journal of Operational Research},
	volume = {274},
	number = {2},
	pages = {654-670},
	year = {2019},
	issn = {0377-2217},
	doi = {https://doi.org/10.1016/j.ejor.2018.10.003},
	author = {Silvana M. Pesenti and Pietro Millossovich and Andreas Tsanakas},
	keywords = {Robustness and sensitivity analysis, Risk management, Value-at-Risk, Expected Shortfall, Stress testing},
	abstract = {Sensitivity analysis is an important component of model building, interpretation and validation. A model comprises a vector of random input factors, an aggregation function mapping input factors to a random output, and a (baseline) probability measure. A risk measure, such as Value-at-Risk and Expected Shortfall, maps the distribution of the output to the real line. As is common in risk management, the value of the risk measure applied to the output is a decision variable. Therefore, it is of interest to associate a critical increase in the risk measure to specific input factors. We propose a global and model-independent framework, termed ‘reverse sensitivity testing’, comprising three steps: (a) an output stress is specified, corresponding to an increase in the risk measure(s); (b) a (stressed) probability measure is derived, minimising the Kullback–Leibler divergence with respect to the baseline probability, under constraints generated by the output stress; (c) changes in the distributions of input factors are evaluated. We argue that a substantial change in the distribution of an input factor corresponds to high sensitivity to that input and introduce a novel sensitivity measure to formalise this insight. Implementation of reverse sensitivity testing in a Monte Carlo setting can be performed on a single set of input/output scenarios, simulated under the baseline model. Thus the approach circumvents the need for additional computationally expensive evaluations of the aggregation function. We illustrate the proposed approach through numerical examples with a simple insurance portfolio and a model of a London Insurance Market portfolio used in industry.}
}

@article{pesenti_bettini_millossovich_tsanakas_2021, 
	title={Scenario Weights for Importance Measurement (SWIM) – an R package for sensitivity analysis}, 
	volume={15}, 
	DOI={10.1017/S1748499521000130}, 
	number={2}, 
	journal={Annals of Actuarial Science}, 
	publisher={Cambridge University Press}, 
	author={Pesenti, Silvana M. and Bettini, Alberto and Millossovich, Pietro and Tsanakas, Andreas}, 
	year={2021}, 
	pages={458–483},
	abstract = {The Scenario Weights for Importance Measurement (SWIM) package implements a flexible sensitivity analysis framework, based primarily on results and tools developed by Pesenti et al. (2019). SWIM provides a stressed version of a stochastic model, subject to model components (random variables) fulfilling given probabilistic constraints (stresses). Possible stresses can be applied on moments, probabilities of given events, and risk measures such as Value-At-Risk and Expected Shortfall. SWIM operates upon a single set of simulated scenarios from a stochastic model, returning scenario weights, which encode the required stress and allow monitoring the impact of the stress on all model components. The scenario weights are calculated to minimise the relative entropy with respect to the baseline model, subject to the stress applied. As well as calculating scenario weights, the package provides tools for the analysis of stressed models, including plotting facilities and evaluation of sensitivity measures. SWIM does not require additional evaluations of the simulation model or explicit knowledge of its underlying statistical and functional relations; hence, it is suitable for the analysis of black box models. The capabilities of SWIM are demonstrated through a case study of a credit portfolio model.}
}

@article{pesenti_millossovich_tsanakas_2021,
	author = {Pesenti, Silvana M. and Millossovich, Pietro and Tsanakas, Andreas},
	title = {Cascade Sensitivity Measures},
	journal = {Risk Analysis},
	volume = {41},
	number = {12},
	pages = {2392-2414},
	keywords = {dependence, importance measures, model uncertainty, risk measures, Rosenblatt transform, Sensitivity analysis},
	doi = {https://doi.org/10.1111/risa.13758},
	abstract = {In risk analysis, sensitivity measures quantify the extent to which the probability distribution of a model output is affected by changes (stresses) in individual random input factors. For input factors that are statistically dependent, we argue that a stress on one input should also precipitate stresses in other input factors. We introduce a novel sensitivity measure, termed cascade sensitivity, defined as a derivative of a risk measure applied on the output, in the direction of an input factor. The derivative is taken after suitably transforming the random vector of inputs, thus explicitly capturing the direct impact of the stressed input factor, as well as indirect effects via other inputs. Furthermore, alternative representations of the cascade sensitivity measure are derived, allowing us to address practical issues, such as incomplete specification of the model and high computational costs. The applicability of the methodology is illustrated through the analysis of a commercially used insurance risk model.},
	year = {2021}
}

@article{jaimungal_pesenti_wang_tatsat_2022,
	author = {Jaimungal, Sebastian and Pesenti, Silvana M. and Wang, Ye Sheng and Tatsat, Hariom},
	title = {Robust Risk-Aware Reinforcement Learning},
	journal = {SIAM Journal on Financial Mathematics},
	volume = {13},
	number = {1},
	pages = {213-226},
	year = {2022},
	doi = {10.1137/21M144640X},
	abstract = {We present a reinforcement learning (RL) approach for robust optimization of risk-aware performance criteria. To allow agents to express a wide variety of risk-reward profiles, we assess the value of a policy using rank dependent expected utility (RDEU). RDEU allows agents to seek gains, while simultaneously protecting themselves against downside risk. To robustify optimal policies against model uncertainty, we assess a policy not by its distribution but rather by the worst possible distribution that lies within a Wasserstein ball around it. Thus, our problem formulation may be viewed as an actor/agent choosing a policy (the outer problem) and the adversary then acting to worsen the performance of that strategy (the inner problem). We develop explicit policy gradient formulae for the inner and outer problems and show their efficacy on three prototypical financial problems: robust portfolio allocation, benchmark optimization, and statistical arbitrage. }
}

@article{bernard_pesenti_vanduffel_2020,
  title={Robust Distortion Risk Measures},
  author={Bernard, Carole and Pesenti, Silvana M. and Vanduffel, Steven},
  journal={Available at SSRN 3677078},
  year={2020},
  url={https://ssrn.com/abstract=3677078},
  abstract = {The robustness of risk measures to changes in underlying loss distributions (distributional uncertainty) is of crucial importance in making well-informed decisions. In this paper, we quantify, for the class of distortion risk measures with an absolutely continuous distortion function, its robustness to distributional uncertainty by deriving its largest (smallest) value when the underlying loss distribution has a known mean and variance and, furthermore, lies within a ball - specified through the Wasserstein distance - around a reference distribution. We employ the technique of isotonic projections to provide for these distortion risk measures a complete characterisation of sharp bounds on their value, and we obtain quasi-explicit bounds in the case of Value-at-Risk and Range-Value-at-Risk. We extend our results to account for uncertainty in the first two moments and provide applications to portfolio optimisation and to model risk assessment.}
}

@article{pesenti_jaimungal_2021,
  title={Portfolio Optimisation within a Wasserstein Ball},
  author={Pesenti, Silvana M. and Jaimungal, Sebastian},
  journal={Available at SSRN 3744994},
  year={2021},
  url={https://ssrn.com/abstract=3744994},
  abstract = {We study the problem of active portfolio management where an investor aims to outperform a benchmark strategy's risk profile while not deviating too far from it. Specifically, an investor considers alternative strategies whose terminal wealth lie within a Wasserstein ball surrounding a benchmark's -- being distributionally close -- and that have a specified dependence/copula -- tying state-by-state outcomes -- to it. The investor then chooses the alternative strategy that minimises a distortion risk measure of terminal wealth. In a general (complete) market model, we prove that an optimal dynamic strategy exists and provide its characterisation through the notion of isotonic projections. We further propose a simulation approach to calculate the optimal strategy's terminal wealth, making our approach applicable to a wide range of market models. Finally, we illustrate how investors with different copula and risk preferences invest and improve upon the benchmark using the Tail Value-at-Risk, inverse S-shaped, and lower- and upper-tail distortion risk measures as examples. We find that investors' optimal terminal wealth distribution has larger probability masses in regions that reduce their risk measure relative to the benchmark while preserving the benchmark's structure.}
}

@article{pesenti_wang_wang_2021,
  title={Optimizing Distortion Riskmetrics With Distributional Uncertainty},
  author={Pesenti, Silvana M. and Wang, Qiuqi and Wang, Ruodu},
  journal={Available at SSRN 3728638},
  year={2021},
  url={https://ssrn.com/abstract=3728638},
  abstract = {Optimization of distortion riskmetrics with distributional uncertainty has wide applications in finance and operations research. Distortion riskmetrics include many commonly applied risk measures and deviation measures, which are not necessarily monotone or convex. One of our central findings is a unifying result that allows us to convert an optimization of a non-convex distortion riskmetric with distributional uncertainty to a convex one, leading to great tractability. A sufficient condition to the unifying equivalence result is the novel notion of closedness under concentration, a variation of which is also shown to be necessary for the equivalence. Our results include many special cases that are well studied in the optimization literature, including but not limited to optimizing probabilities, Value-at-Risk, Expected Shortfall, Yaari's dual utility, and differences between distortion risk measures, under various forms of distributional uncertainty. We illustrate our theoretical results via applications to portfolio optimization, optimization under moment constraints, and preference robust optimization.}
}

@article{ince_peri_pesenti_2021,
  title={Risk Contributions of Lambda Quantiles},
  author={Ince, Akif and Peri, Ilaria and Pesenti, Silvana M.},
  journal={Available at SSRN 3874970},
  year={2022},
  url={https://ssrn.com/abstract=3874970},
  abstract = {Risk contributions of portfolios form an indispensable part of risk adjusted performance measurement. The risk contribution of a portfolio, e.g., in the Euler or Aumann-Shapley framework, is given by the partial derivatives of a risk measure applied to the portfolio profit and loss in direction of the asset units. For risk measures that are not positively homogeneous of degree 1, however, known capital allocation principles do not apply. We study the class of lambda quantile risk measures that includes the well-known Value-at-Risk as a special case but for which no known allocation rule is applicable. We prove differentiability and derive explicit formulae of the derivatives of lambda quantiles with respect to their portfolio composition, that is their risk contribution. For this purpose, we define lambda quantiles on the space of portfolio compositions and consider generic (also non-linear) portfolio operators.  We further derive the Euler decomposition of lambda quantiles for generic portfolios and show that lambda quantiles are homogeneous in the space of portfolio compositions, with a homogeneity degree that depends on the portfolio composition and the lambda function. This result is in stark contrast to the positive homogeneity properties of risk measures defined on the space of random variables which admit a constant homogeneity degree. We introduce a generalised version of Euler contributions and Euler allocation rule, which are compatible with risk measures of any homogeneity degree and non-linear but homogeneous portfolios. These concepts are illustrated by a non-linear portfolio using financial market data.}
}

@article{dacosta_pesenti_targino_2021,
  title={Risk Budgeting Portfolios from Simulations},
  author={Freitas Paulo da Costa, Bernardo and Pesenti, Silvana M. and Targino, Rodrigo},
  journal={Available at SSRN 4038514},
  year={2022},
  url={https://ssrn.com/abstract=4038514},
  abstract = {Risk budgeting is a portfolio strategy where each asset contributes a prespecified amount to the aggregate risk of the portfolio. In this work, we propose a numerical framework that uses only simulations of returns for estimating risk budgeting portfolios. Specifically, we provide a Sample Average Approximation (SAA) algorithm with cutting planes, and a Stochastic Gradient Decent (SGD) algorithm, tailored to the risk budgeting portfolio for the Expected Shortfall. We illustrate different risk budgeting portfolios, constructed using a especially designed Julia package, on real financial data and compare it to classical portfolio strategies.}
}

@article{fissler_pesenti_2022,
  title={Sensitivity Measures Based on Scoring Functions},
  author={Fissler, Tobias and Pesenti, Silvana M.},
  journal={Available at SSRN 4046894},
  year={2022},
  url={https://ssrn.com/abstract=4046894},
  abstract = {We propose a holistic framework for constructing sensitivity measures for any elicitable functional T of a response variable. The sensitivity measures, termed score-based sensitivities, are constructed via scoring functions that are (strictly) consistent for T. These score-based sensitivities quantify the relative improvement in predictive accuracy when available information, e.g., from explanatory variables, is used ideally. We establish intuitive and desirable properties of these sensitivities and discuss advantageous choices of scoring functions leading to scale-invariant sensitivities.  Since elicitable functionals typically possess rich classes of (strictly) consistent scoring functions, we demonstrate how Murphy diagrams can provide a picture of all score-based sensitivity measures. We discuss the family of score-based sensitivities for the mean functional (of which the Sobol indices are a special case) and risk functionals such as Value-at-Risk, and the pair Value-at-Risk and Expected Shortfall. The sensitivity measures are illustrated using numerous examples, including the Ishigami--Homma test function. In a simulation study, estimation of score-based sensitivities for a non-linear insurance portfolio is performed using neural nets.}
}

@article{chi_jaimungal_lin_2010,
	title = {An insurance risk model with stochastic volatility},
	journal = {Insurance: Mathematics and Economics},
	volume = {46},
	number = {1},
	pages = {52-66},
	year = {2010},
	note = {Gerber–Shiu Functions / Longevity risk and capital markets},
	issn = {0167-6687},
	doi = {https://doi.org/10.1016/j.insmatheco.2009.03.006},
	author = {Yichun Chi and Sebastian Jaimungal and X. Sheldon Lin},
	keywords = {Gerber–Shiu expected discounted penalty function, Integro-differential equation, Singular perturbation theory, Stochastic volatility, Perturbed compound Poisson risk process, Phase-type distribution, Ornstein–Uhlenbeck process},
	abstract = {In this paper, we extend the Cramér–Lundberg insurance risk model perturbed by diffusion to incorporate stochastic volatility and study the resulting Gerber–Shiu expected discounted penalty (EDP) function. Under the assumption that volatility is driven by an underlying Ornstein–Uhlenbeck (OU) process, we derive the integro-differential equation which the EDP function satisfies. Not surprisingly, no closed-form solution exists; however, assuming the driving OU process is fast mean-reverting, we apply the singular perturbation theory to obtain an asymptotic expansion of the solution. Two integro-differential equations for the first two terms in this expansion are obtained and explicitly solved. When the claim size distribution is of phase-type, the asymptotic results simplify even further and we succeed in estimating the error of the approximation. Hyper-exponential and mixed-Erlang distributed claims are considered in some detail.}
}

@article{chi_lin_2011,
	title = {On the threshold dividend strategy for a generalized jump–diffusion risk model},
	journal = {Insurance: Mathematics and Economics},
	volume = {48},
	number = {3},
	pages = {326-337},
	year = {2011},
	issn = {0167-6687},
	doi = {https://doi.org/10.1016/j.insmatheco.2010.11.006},
	author = {Yichun Chi and X. Sheldon Lin},
	keywords = {Jump–diffusion risk model, Expected discounted penalty function, Threshold dividend strategy, Wiener–Hopf factorization},
	abstract = {In this paper, we generalize the Cramér–Lundberg risk model perturbed by diffusion to incorporate jumps due to surplus fluctuation and to relax the positive loading condition. Assuming that the surplus process has exponential upward and arbitrary downward jumps, we analyze the expected discounted penalty (EDP) function of Gerber and Shiu (1998) under the threshold dividend strategy. An integral equation for the EDP function is derived using the Wiener–Hopf factorization. As a result, an explicit analytical expression is obtained for the EDP function by solving the integral equation. Finally, phase-type downward jumps are considered and a matrix representation of the EDP function is presented.}
}

@article{chi_lin_2012, 
	title={Are Flexible Premium Variable Annuities Under-Priced?}, 
	volume={42}, 
	DOI={10.2143/AST.42.2.2182808}, 
	number={2}, 
	journal={ASTIN Bulletin}, 
	publisher={Cambridge University Press}, 
	author={Chi, Yichun and Lin, X. Sheldon}, 
	year={2012}, 
	pages={559–574},
	abstract = {A variable annuity (VA) is a deferred annuity that allows an annuitant to invest his/her contributions into a range of mutual funds. A separate account termed as sub-account is set up for the investment. Unlike a mutual fund, a VA offers a guaranteed minimum death benefit or GMDB and often offers a guaranteed minimum living benefit or GMLB during the accumulation phase of the VA contract. Almost all the research to date has focused on single premium variable annuities (SPVAs), i.e. it is assumed that an annuitant makes a single lump-sum contribution at the time of issue. In this paper, we study flexible premium variable annuities (FPVAs) that allow contributions during the accumulation phase. We derive a valuation formula for guarantees embedded in FPVAs and show that the delta hedging strategy for an FPVA is substantially different from that for an SPVA. The numerical examples illustrate that the cost in the form of mortality and expense (M&E) fee for an FPVA in many situations is significantly higher than the cost for a similar SPVA. This finding suggests that the current pricing practice by most VA providers that charges the same M&E fee for both should be re-examined.}
}

@article{chi_lin_2014, 
	title={Optimal reinsurance with limited ceded risk: A stochastic dominance approach}, 
	volume={44}, 
	DOI={10.1017/asb.2013.28}, 
	number={1}, 
	journal={ASTIN Bulletin}, 
	publisher={Cambridge University Press}, 
	author={Chi, Yichun and Lin, X. Sheldon}, 
	year={2014}, 
	pages={103–126},
	abstract = {An optimal reinsurance problem from the perspective of an insurer is studied in this paper, where an upper limit is imposed on a reinsurer's expected loss over a prescribed level. In order to reduce the moral hazard, we assume that both the insurer and the reinsurer are obligated to pay more as the amount of loss increases in a typical reinsurance treaty. We further assume that the optimization criterion preserves the convex order. Such a criterion is very general as most of the criteria for optimal reinsurance problems in the literature preserve the convex order. When the reinsurance premium is calculated as a function of the actuarial value of coverage, we show via a stochastic dominance approach that any admissible reinsurance policy is dominated by a stop-loss reinsurance or a two-layer reinsurance, depending upon the amount of the reinsurance premium. Moreover, we obtain a similar result to Mossin's Theorem and find that it is optimal for the insurer to cede a loss as much as possible under the net premium principle. To further examine the reinsurance premium for the optimal piecewise linear reinsurance policy, we assume the expected value premium principle and derive the optimal reinsurance explicitly under (1) the criterion of minimizing the variance of the insurer's risk exposure, and (2) the criterion of minimizing the risk-adjusted value of the insurer's liability where the liability valuation is carried out using the cost-of-capital approach based on the conditional value at risk.}
}

@article{chi_lin_tan_2017,
	author = {Yichun Chi and X. Sheldon Lin and Ken Seng Tan},
	title = {Optimal Reinsurance Under the Risk-Adjusted Value of an Insurer’s Liability and an Economic Reinsurance Premium Principle},
	journal = {North American Actuarial Journal},
	volume = {21},
	number = {3},
	pages = {417-432},
	year  = {2017},
	publisher = {Routledge},
	doi = {10.1080/10920277.2017.1302346},
	abstract = { In this article, an optimal reinsurance problem is formulated from the perspective of an insurer, with the objective of minimizing the risk-adjusted value of its liability where the valuation is carried out by a cost-of-capital approach and the capital at risk is calculated by either the value-at-risk (VaR) or conditional value-at-risk (CVaR). In our reinsurance arrangement, we also assume that both insurer and reinsurer are obligated to pay more for a larger realization of loss as a way of reducing ex post moral hazard. A key contribution of this article is to expand the research on optimal reinsurance by deriving explicit optimal reinsurance solutions under an economic premium principle. It is a rather general class of premium principles that includes many weighted premium principles as special cases. The advantage of adopting such a premium principle is that the resulting reinsurance premium depends not only on the risk ceded but also on a market economic factor that reflects the market environment or the risk the reinsurer is facing. This feature appears to be more consistent with the reinsurance market. We show that the optimal reinsurance policies are piecewise linear under both VaR and CVaR risk measures. While the structures of optimal reinsurance solutions are the same for both risk measures, we also formally show that there are some significant differences, particularly on the managing tail risk. Because of the integration of the market factor (via the reinsurance pricing) into the optimal reinsurance model, some new insights on the optimal reinsurance design could be gleaned, which would otherwise be impossible for many of the existing models. For example, the market factor has a nontrivial effect on the optimal reinsurance, which is greatly influenced by the changes of the joint distribution of the market factor and the loss. Finally, under an additional assumption that the market factor and the loss have a copula with quadratic sections, we demonstrate that the optimal reinsurance policies admit relatively simple forms to foster the applicability of our theoretical results, and a numerical example is presented to further highlight our results. }
}

@article{tseung_chan_fung_badescu_lin_2023,
	author = {Tseung, Spark C. and Chan, Ian Weng and Fung, Tsz Chai and Badescu, Andrei L. and Lin, X. Sheldon},
	title = {Improving risk classification and ratemaking using mixture-of-experts models with random effects},
	journal = {Journal of Risk and Insurance},
	volume = {n/a},
	number = {n/a},
	pages = {},
	keywords = {mixture-of-experts, random effects, ratemaking, risk classification, variational inference},
	doi = {https://doi.org/10.1111/jori.12436},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/jori.12436},
	abstract = {In the underwriting and pricing of nonlife insurance products, it is essential for the insurer to utilize both policyholder information and claim history to ensure profitability and proper risk management. In this paper, we apply a flexible regression model with random effects, called the Mixed Logit-weighted Reduced Mixture-of-Experts, which leverages both policyholder information and their claim history, to categorize policyholders into groups with similar risk profiles, and to determine a premium that accurately captures the unobserved risks. Estimates of model parameters and the posterior distribution of random effects can be obtained by a stochastic variational algorithm, which is numerically efficient and scalable to large insurance portfolios. Our proposed framework is shown to outperform the classical benchmark models (Logistic and Lognormal GL(M)M) in terms of goodness-of-fit to data, while offering intuitive and interpretable characterization of policyholders' risk profiles to adequately reflect their claim history.}
}

@article{fung_tseung_2022,
  doi = {10.48550/ARXIV.2209.15207},
  author = {Fung, Tsz Chai and Tseung, Spark C.},
  keywords = {Statistics Theory (math.ST), Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Methodology (stat.ME), FOS: Mathematics, FOS: Mathematics, FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Mixture of experts models for multilevel data: modelling framework and approximation theory},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license},
  abstract = {Multilevel data are prevalent in many real-world applications. However, it remains an open research problem to identify and justify a class of models that flexibly capture a wide range of multilevel data. Motivated by the versatility of the mixture of experts (MoE) models in fitting regression data, in this article we extend upon the MoE and study a class of mixed MoE (MMoE) models for multilevel data. Under some regularity conditions, we prove that the MMoE is dense in the space of any continuous mixed effects models in the sense of weak convergence. As a result, the MMoE has a potential to accurately resemble almost all characteristics inherited in multilevel data, including the marginal distributions, dependence structures, regression links, random intercepts and random slopes. In a particular case where the multilevel data is hierarchical, we further show that a nested version of the MMoE universally approximates a broad range of dependence structures of the random effects among different factor levels.}
}

@article{calcetero_badescu_lin_2022,
  title={A Credibility Index Approach for Effective a Posteriori Ratemaking with Large Insurance Portfolios},
  author={Calcetero Vanegas, Sebastian and Badescu, Andrei and Lin, X. Sheldon},
  journal={Available at SSRN 4275353},
  year={2022},
  url={https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4275353},
  abstract={Credibility, experience rating and more recently the so-called a posterior ratemaking in insurance consists in the determination of premiums that account for both the policyholders' attributes and their claim history. The models designed for such purposes are known as credibility models and fall under the same framework of Bayesian inference in statistics. Most of the data-driven models used for this task are mathematically intractable due to their complex structure, and therefore credibility premiums must be obtained via numerical methods e.g simulation via Markov Chain Monte Carlo. However, such methods are computationally expensive and even prohibitive for large portfolios when these must be applied at the policyholder level. In addition, these computations are ``black-box" procedures for actuaries as there is no clear expression showing how the claim history of policyholders is used to upgrade their premiums. In this paper, we address these challenges and propose a methodology to derive a closed-form expression to compute credibility premiums for any given Bayesian model. We do so by introducing a credibility index, that works as an efficient summary statistic of the claim history of a policyholder, and illustrate how it can be used as the main input to approximate any credibility formula. The closed-form solution can be used to reduce the computational burden of a posteriori ratemaking for large portfolios via the same idea of surrogate modeling, and also provides a transparent way of computing premiums from which practical interpretations and risk assessments can be performed.}
}


@article{chan_tseung_badescu_lin_2023,
  doi = {10.48550/arXiv.2304.10591},
  author = {Chan, Ian Weng and Tseung, Spark C. and Badescu, Andrei L. and Lin, X. Sheldon},
  keywords = {Applications (stat.AP)},
  title = {Data Mining of Telematics Data: Unveiling the Hidden Patterns in Driving Behaviour},
  publisher = {arXiv},
  year = {2023},
  copyright = {arXiv.org perpetual, non-exclusive license},
  abstract = {With the advancement in technology, telematics data which capture vehicle movements information are becoming available to more insurers.  As these data capture the actual driving behaviour, they are expected to improve our understanding of driving risk and facilitate more accurate auto-insurance ratemaking.  In this paper, we analyze an auto-insurance dataset with telematics data collected from a major European insurer.  Through a detailed discussion of the telematics data structure and related data quality issues, we elaborate on practical challenges in processing and incorporating telematics information in loss modelling and ratemaking.  Then, with an exploratory data analysis, we demonstrate the existence of heterogeneity in individual driving behaviour, even within the groups of policyholders with and without claims, which supports the study of telematics data.  Our regression analysis reiterates the importance of telematics data in claims modelling; in particular, we propose a speed transition matrix that describes discretely recorded speed time series and produces statistically significant predictors for claim counts.  We conclude that large speed transitions, together with higher maximum speed attained, nighttime driving and increased harsh braking, are associated with increased claim counts.  Moreover, we empirically illustrate the learning effects in driving behaviour: we show that both severe harsh events detected at a high threshold and expected claim counts are not directly proportional with driving time or distance, but they increase at a decreasing rate.}
}

@article{calcetero_badescu_lin_2023,
  title={Claim Reserving via Inverse Probability Weighting: A Micro-Level Chain-Ladder Method},
  author={Calcetero Vanegas, Sebastian and Badescu, Andrei and Lin, X. Sheldon},
  journal={Available at SSRN 4499355},
  year={2023},
  url={https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4499355},
  abstract={Claim reserving is primarily accomplished using macro-level or aggregate models, with the Chain-Ladder method being the most popular one. However, these methods are heuristically constructed, rely on oversimplified data assumptions, neglect the heterogeneity of policyholders, and so lead to a lack of accuracy. In contrast, micro-level reserving leverages on stochastic modeling with granular information for improved predictions, but usually comes at the cost of more complex models that are unattractive to practitioners. In this paper, we introduce a simplistic macro-level type approach that can incorporate granular information at the individual level. We do so by considering a novel framework in which we view the claim reserving problem as a population sampling problem and propose an estimator using inverse probability weighting techniques, with weights driven by policyholder attributes. The framework provides a statistically sound method for aggregate claim reserving in a frequency and severity distribution-free fashion, while also incorporating the capability to utilize granular information via a regression-type framework. The resulting reserve estimator has the attractiveness of resembling the Chain-Ladder claim development principle but applied at the individual claim level, so it is easy to interpret by actuaries, and more appealing to practitioners as an extension of a macro-model.}
}